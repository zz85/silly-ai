# Silly AI Configuration

name = "Silly"
wake_word = "Hey Silly"
wake_timeout_secs = 30

# LLM Configuration
[llm]
backend = "llama-cpp"

# Model from HuggingFace (auto-downloads on first run)
hf_repo = "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"
hf_file = "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
prompt_format = "chatml"  # chatml, mistral, or llama3

# Or use a local GGUF file:
# model_path = "models/your-model.gguf"

# Popular models:
# TinyLlama (fast, ~670MB):
#   hf_repo = "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"
#   hf_file = "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
#   prompt_format = "chatml"
#
# Mistral 7B (quality, ~4GB):
#   hf_repo = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
#   hf_file = "mistral-7b-instruct-v0.2.Q4_K_M.gguf"
#   prompt_format = "mistral"
#
# Llama 3 8B (quality, ~4.5GB):
#   hf_repo = "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF"
#   hf_file = "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"
#   prompt_format = "llama3"

# =============================================================================
# OpenAI-Compatible API (Recommended for API-based inference)
# =============================================================================
# Works with: LM Studio, OpenAI, Ollama (via API), Together.ai, Groq, etc.

# Option 1: Use preset for common providers
# [llm]
# backend = "openai-compat"
# preset = "lm_studio"  # Options: "lm_studio", "openai", "ollama"
# model = "local-model"
# temperature = 0.7
# top_p = 0.9
# max_tokens = 1024

# Option 2: Specify base_url directly
# [llm]
# backend = "openai-compat"
# base_url = "http://localhost:1234"
# model = "your-model"
# temperature = 0.7

# Option 3: Use with OpenAI (requires API key)
# [llm]
# backend = "openai-compat"
# preset = "openai"
# model = "gpt-4"
# api_key = "${OPENAI_API_KEY}"  # Supports environment variables
# temperature = 0.7
# max_tokens = 1024

# =============================================================================
# Provider-Specific Examples
# =============================================================================

# LM Studio (local inference with GUI)
# [llm]
# backend = "openai-compat"
# preset = "lm_studio"  # or base_url = "http://localhost:1234"
# model = "model-name"

# OpenAI
# [llm]
# backend = "openai-compat"
# preset = "openai"
# model = "gpt-4o"
# api_key = "${OPENAI_API_KEY}"

# Together.ai
# [llm]
# backend = "openai-compat"
# base_url = "https://api.together.xyz/v1"
# model = "mistralai/Mixtral-8x7B-Instruct-v0.1"
# api_key = "${TOGETHER_API_KEY}"

# Groq
# [llm]
# backend = "openai-compat"
# base_url = "https://api.groq.com/openai/v1"
# model = "mixtral-8x7b-32768"
# api_key = "${GROQ_API_KEY}"

# LocalAI (self-hosted)
# [llm]
# backend = "openai-compat"
# base_url = "http://localhost:8080/v1"
# model = "model-name"

# =============================================================================
# Ollama Backend (native SDK)
# =============================================================================
# Or use Ollama (requires ollama server running):
# [llm]
# backend = "ollama"
# model = "mistral:7b-instruct"

# TTS Configuration
[tts]
engine = "supertonic"
onnx_dir = "models/supertonic/onnx"
voice_style = "models/supertonic/voice_styles/M1.json"
speed = 1.1  # 0.5 to 2.0

# Or use Kokoro TTS (requires --features kokoro):
# [tts]
# engine = "kokoro"
# model = "models/kokoro-v1.0.onnx"
# voices = "models/voices-v1.0.bin"
# speed = 1.1

# Interaction settings
[interaction]
crosstalk = false  # Continue listening while TTS plays (enables barge-in)
aec = false        # Acoustic echo cancellation (requires --features aec)
duck_volume = 0.2  # TTS volume when user speaks during playback (0.0-1.0)

# Hardware acceleration (CoreML on Apple Silicon)
[acceleration]
tts_gpu = true   # CoreML for TTS
vad_gpu = false  # CoreML for VAD (small model, CPU is fine)

# Voice-to-keyboard typing (requires --features typing)
[typing]
input_method = "direct"     # "direct" (default) or "clipboard" (may have issues on macOS)
feedback = true             # Audio/visual feedback when commands recognized
undo_buffer_size = 50       # Number of operations to track for undo
command_pause_ms = 100      # Min pause (ms) for short phrases to be recognized as commands
stop_phrase = "silly stop"  # Phrase to pause typing mode (use "silly terminate" to exit)

